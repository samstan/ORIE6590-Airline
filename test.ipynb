{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing open ai gym and the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import airline\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import torch as th\n",
    "from GruActorCriticPolicy import CustomActorCriticPolicy as GruACP\n",
    "from LongActorCriticPolicy import CustomActorCriticPolicy as LACP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function for evaluating the model on a given environment for a specified number of episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, numiters):\n",
    "    rewards = []\n",
    "    for j in range(numiters):\n",
    "        obs = env.reset()\n",
    "        tot_reward = 0\n",
    "        for i in range(env.tau):\n",
    "            action, states = model.predict(obs)\n",
    "            _, reward, _, _ = env.step(action)\n",
    "            tot_reward += reward\n",
    "        rewards.append(tot_reward)\n",
    "    return np.mean(rewards), np.std(rewards), np.amax(rewards), np.min(rewards)\n",
    "\n",
    "# Model is the random policy\n",
    "def policy(env):\n",
    "    return env.action_space.sample()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the configuration for the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Environment\n",
    "A = np.asarray([[1, 1, 0,0,0,0], [ 0,0, 1, 1, 1, 1], [ 0,0, 0,0, 1, 1] ])\n",
    "tau = 23\n",
    "P = np.ones((tau, A.shape[1]))/3\n",
    "c = [5, 5, 5]\n",
    "f = range(10, 16)\n",
    "CONFIG = {'A': A, 'f': f, 'P': P, 'starting_state': c , 'tau': tau}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 6\n",
    "l = 3\n",
    "A = np.identity(m)\n",
    "for i in range(l):\n",
    "    for j in range(l):\n",
    "        if i != j:\n",
    "            demand_col = np.zeros((m, 1))\n",
    "            demand_col[2 * i + 1] = 1.0\n",
    "            demand_col[2 * j] = 1.0\n",
    "            A=  np.append(A, demand_col, axis = 1)\n",
    "A = np.append(A, A, axis = 1)\n",
    "tau = 20\n",
    "P = np.array([0.01327884, 0.02244177, 0.07923761, 0.0297121,  0.02654582, 0.08408091, 0.09591975, 0.00671065, 0.08147508, 0.00977341, 0.02966204, 0.121162, 0.00442628, 0.00748059, 0.02641254, 0.00990403, 0.00884861, 0.02802697, 0.03197325, 0.00223688, 0.02715836, 0.0032578,  0.00988735, 0.04038733])\n",
    "P = np.array([P]*tau)\n",
    "c = [2]*6\n",
    "f = np.array([33, 28, 36, 34, 17, 20, 39, 24, 31, 19, 30, 48, 165, 140, 180, 170, 85, 100,195, 120, 155, 95, 150, 240])\n",
    "CONFIG = {'A': A, 'f': f, 'P': P, 'starting_state': c , 'tau': tau}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making an instance of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "env = gym.make('Airline-v0', config=CONFIG)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline MlpPolicy\n",
    "model = A2C(\"MlpPolicy\", env, n_steps = 5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OneSharedLayer\n",
    "policy_kwargs = dict(net_arch=[64, dict(vf=[64, 64], pi=[64, 64])  ])\n",
    "model = A2C(\"MlpPolicy\", env,  policy_kwargs = policy_kwargs, n_steps = 5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gru\n",
    "model = A2C(GruACP, env, n_steps = 5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Possible Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WideMlp\n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=[1024, 1024, 1024 ])\n",
    "model = A2C(\"MlpPolicy\", env,  policy_kwargs = policy_kwargs, n_steps = 5, verbose=1)\n",
    "#DeepMlp\n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch = [32, 32, 64, 64, 128, 256, 128, 64, 64, 32, 32])\n",
    "model = A2C(LongActorCriticPolicy, env,  policy_kwargs = policy_kwargs, n_steps = 5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"a2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating randomized policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 50\n",
    "rewards = []\n",
    "for j in range(e):\n",
    "    obs = env.reset()\n",
    "    tot_reward = 0\n",
    "    for i in range(tau):\n",
    "        action, states_ = model.predict(obs)\n",
    "        obs, reward, dones, info = env.step(action)\n",
    "        tot_reward += reward\n",
    "    rewards.append(tot_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Here is the mean and standard deviation of the estimate ' + str(evaluate(model, env, 500)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
